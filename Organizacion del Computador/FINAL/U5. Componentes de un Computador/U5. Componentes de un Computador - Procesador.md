# U5 - Componentes de un procesador: Procesador

Def. previa: **Ciclo de reloj:** unidad b√°sica que mide la velocidad de una CPU. Durante cada ciclo, miles de millones de transistores dentro del procesador se abren y cierran. La frecuencia de reloj indica la frecuencia a la cual los transistores que lo conforman conmutan el√©ctricamente, es decir, abren y cierran el flujo de una corriente el√©ctrica.

## CISC (Complex Instruction Set Computer)
- Pocos registros de procesador (especializados)
- Set de Instrucciones amplio
- Muchas instrucciones para trabajar con memoria
- Microarquitectura en sofware/hardware compleja
- Instrucciones complejas (m√°s de un ciclo de reloj)
- Varios modos de direccionamiento, muchos tipos de datos, muchos formatos de instrucci√≥n (variables o h√≠bridos)
- Orientado al hardware, compiladores relativamente simples (tama√±o de c√≥digo peque√±o)
- Ejemplos: VAX, Intel x86 (hasta IA -32), Intel-64, IBM Mainframe

## RISC (Reduced Instruction Set Computer)
- Muchos registros de procesador de uso general
- Set de Instrucciones peque√±o
- Solo acceso a memoria a trav√©s de LOAD/STORE
- Microarquitectura en hardware simple
- Instrucciones simples (un ciclo de reloj)
- Pocos modos de direccionamiento, pocos tipos de datos, pocos formatos de instrucci√≥n (fijos)
- Orientado al software, compiladores relativamente complejos (tama√±o de c√≥digo largo)
- Ejemplos: SPARC, MIPS, ARM, Intel Itanium (IA-64)

## CISC vs RISC
Como los procesadores CISC tienen un conjunto de instrucciones m√°s complejo, pueden ejecutar programas de manera m√°s eficiente en t√©rminos de c√≥digo. En cambia, porque los procesadores RISC tienen un conjunto de instrucciones m√°s simple, tienden a ser m√°s r√°pidos y eficientes en t√©rminos de energ√≠a.

Mientras que los procesadores CISC pueden ser m√°s adecuados para sistemas con limitaciones de memoria y donde la compatibilidad con software existente es cr√≠tica, los procesadores RISC ofrecen ventajas en t√©rminos de velocidad, eficiencia energ√©tica y simplicidad de dise√±o, haci√©ndolos ideales para dispositivos m√≥viles y sistemas embebidos donde estos factores son prioritarios.

# Arquitectura de procesadores

## Ecuacion de *performance*
- MIPS = Millions of Instructions Per Second.
- An approximate measure of a computer's raw processing power

`MIPS rate = Frecuencia del reloj en MHz (f) * Instrucciones por ciclo (IPC)`

## Uniprocesadores (procesadores con un solo nucleo)
Un solo procesador central (CPU) que ejecuta todas las instrucciones y procesos.

Estos procesadores mejoraban ‚úîÔ∏è su rendimiento principalmente aumentando la velocidad del ciclo de reloj ‚è∞, es decir, incrementando la cantidad de ciclos de reloj que el CPU pod√≠a manejar.

üî¥ Una limitacion para seguir aumentando la velocidad es la generacion de calor üî• : trabajar üßë‚Äçüè≠ a mayor velocidad implica una generaci√≥n de calor m√°s alta debido a la mayor actividad de los transistores. Esto llev√≥ a que los primeros CPUs no necesitaran disipadores de calor, pero conforme se incremento la velocidad de los procesadores, se volvi√≥ necesario a√±adir disipadores de calor.

Hasta los 2000, la mejora continua de la velocidad de los procesadores seguia una tendencia exponencial üìà pero despues de ese punto se llego a un limite practico. En las ultimas dos decadas la frecuencia de los procesadores no ha experimentado un crecimiento tan significativo como en d√©cadas anteriores. A partir de aca los fabricantes se centraron en otras √°reas de mejora del rendimiento, como la eficiencia energ√©tica üîã, el dise√±o de n√∫cleos m√∫ltiples y la optimizaci√≥n del hardware üñ•Ô∏è y el software en general üò∏.

Hasta ese momento de limite se mejoraba la primera parte de la ecuacion de performance (la *frecuencia de reloj*)

## Tecnicas de procesamiento paralelo (Paralelismo)
Estas t√©cnicas permiten aumentar la eficiencia y reducir el tiempo necesario para completar una tarea, ya que las operaciones no se ejecutan secuencialmente sino en paralelo. 

Es un enfoque que ahora en vez de pedirle al CPU que ejecute mas instrucciones (para tener mayor *frecuencia de reloj**, mejorando la primera parte de la ecuacion), ahora se enfoca en implementar tecnicas para que el CPU ejecute mas instrucciones por ciclo de reloj (mejorando la segunda parte de la ecuacion)

Se pueden diferencias tecnicas con dos enfoques distintos:
- **A nivel instruccion:** pipelining, dual pipelining, superscalar, multithreading
- **A nivel procesador:** procesadores paralelos de datos, multiprocesadores, multicomputadoras

### Pipelining
- El CPU trata de ejecutar en paralelo mas de una instruccion de maquina, y con el pipelining lo logra solapando la ejecucion de las instrucciones: se permite que la ejecuci√≥n de una instrucci√≥n comience antes de que se complete la ejecuci√≥n de la instrucci√≥n anterior, y asi se logra reducir el tiempo total de una secuencia de instrucciones. **Se divide el ciclo de instruccion en etapas**
- Ejecucion de una instruccion por ciclo de reloj: cada etapa del pipeline est√° dise√±ada para completarse en un ciclo de reloj. Esto significa que, idealmente, una instrucci√≥n se ejecuta completamente en un solo ciclo de reloj.

![img](https://github.com/user-attachments/assets/e2838bab-6d76-4407-bbeb-1c62a72f3bf1) <br>
*A partir del t=5 se puede ver que hay cinco instrucciones siendo procesadas en paralelo*

#### Etapas 
En el hardware hay una unidad dedicada para cada etapa, cada unidad se va a llenar con la instruccion de maquina que este pasando por esa etapa. Las etapas son:
1. **Fetch:** leer la instrucci√≥n siguiente desde la memoria del programa y cargarla en el procesador. La direcci√≥n de la pr√≥xima instrucci√≥n a ejecutar se mantiene en el contador de programa (PC), que se actualiza despu√©s de cada captura.
2. **Decode**: la instrucci√≥n capturada se analiza para determinar qu√© acci√≥n debe realizar el procesador. Esto incluye identificar el c√≥digo de operaci√≥n (opcode) y los registros o ubicaciones de memoria involucrados.
3. **Operand Fetch**: el procesador recupera los operandos necesarios para la ejecuci√≥n de la instrucci√≥n. Los operandos pueden estar en registros dentro del procesador o en la memoria.
4. **Instruction Execution**: es donde se realiza la operaci√≥n especificada por la instrucci√≥n.
5. **Write Back**: los resultados de la ejecuci√≥n de la instrucci√≥n se escriben de vuelta en el lugar adecuado.

El pipelining permite que estas etapas se solapen en el tiempo; es decir, mientras una instrucci√≥n est√° siendo ejecutada, otra puede estar siendo decodificada, y una tercera puede estar siendo capturada.

#### Control de dependencia entre las instrucciones
Las instrucciones que entran al pipeline podrian utilizar datos no actualizados por una instruccion previa que no se termino de ejecutar. Para garantizar que las instrucciones se ejecuten correctamente en el pipeline y evitar problemas el pipelining requiere un control de dependencia entre las instrucciones.

Tipos de dependencias:
- Dependencias de datos: Ocurren cuando una instrucci√≥n depende de los resultados de otra. Por ejemplo, si una instrucci√≥n necesita el resultado de la instrucci√≥n anterior para ejecutarse correctamente.
- Dependencias de control: Se dan cuando el flujo de instrucciones depende del resultado de operaciones de control, como las bifurcaciones condicionales (if-else). Esto puede hacer que sea incierto qu√© instrucci√≥n debe ser cargada en el pipeline a continuaci√≥n.
- Dependencias de recursos: Suceden cuando dos o m√°s instrucciones necesitan el mismo recurso (como un registro o unidad de ejecuci√≥n) al mismo tiempo.

Para manejar estas dependencias y minimizar los retrasos (stalls) y las condiciones de peligro (hazards), se utilizan varias t√©cnicas mediante t√©cnicas de hardware o mediante optimizaciones realizadas por el compilador.
